{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-8B, Efficiently scale distributed training Llama 3, PyTorch FSDP and Q-Lora\n",
    "\n",
    "Open LLMs like Meta [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-70b), Mistral AI [Mistral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) & [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models or AI21 [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1) are now OpenAI competitors. However, most of the time you need to fine-tune the model on your data to unlock the full potential of the model. Fine-tuning smaller LLMs, like Mistral became very accessible on a single GPU by using Q-Lora. But efficiently fine-tuning bigger models like Llama 3 70b or Mixtral stayed a challenge until now. \n",
    "\n",
    "This blog post walks you thorugh how to fine-tune a Llama 3 using PyTorch FSDP and Q-Lora with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index). In addition to FSDP we will use [Flash Attention v2 through the Pytorch SDPA](https://pytorch.org/blog/pytorch2-2/) implementation. \n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Create and prepare the dataset](#2-create-and-prepare-the-dataset)\n",
    "3. [Fine-tune the LLM with PyTorch FSDP, Q-Lora and SDPA](#3-fine-tune-the-llm-with-pytorch-fsdp-q-lora-and-sdpa)\n",
    "4. [Test Model and run Inference](#4-test-model-and-run-inference)\n",
    "\n",
    "_Note: This blog was created and validated on NVIDIA H100 and NVIDIA A10G GPUs. The configurations and code is optimized for 4xA10G GPUs each with 24GB of Memory. I hope this keeps the example as accessible as possible for most people. If you have access to more compute you can make changes to the config (yaml) at step 3._\n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "In a collaboration between [Answer.AI](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html), Tim Dettmers [Q-Lora creator](https://github.com/TimDettmers/bitsandbytes) and [Hugging Face](https://huggingface.co/), we are proud to announce to share the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allows you now to fine-tune Llama 2 70b or Mixtral 8x7B on 2x consumer GPUs (24GB). If you want to learn more about the background of this collaboration take a look at [You can now train a 70b language model at home](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html). Hugging Face PEFT is were the magic happens for this happens, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_DATASETS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_HOME'] = \"/home/ec2-user/SageMaker/.cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries and Pyroch, including trl, transformers and datasets. If you haven't heard of trl yet, don't worry. It is a new library on top of transformers and datasets, which makes it easier to fine-tune, rlhf, align open LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Install Pytorch for FSDP and FA/SDPA\n",
    "# %pip install \"torch==2.2.2\" tensorboard\n",
    "\n",
    "# # Install Hugging Face libraries\n",
    "# %pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to login into Hugging Face to access the Llama 3 70b model. If you don't have an account yet and accepted the terms, you can create one [here](https://huggingface.co/join). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ec2-user/SageMaker/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    " ! huggingface-cli login --token \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "After our environment is set up, we can start creating and preparing our dataset. A fine-tuning dataset should have a diverse set of demonstrations of the task you want to solve. If you want to learn more about how to create a dataset, take a look at the [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset).\n",
    "\n",
    "We will use the [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. This data can be used for supervised fine-tuning (SFT) to make language models follow instructions better. No Robots was modelled after the instruction dataset described in OpenAI's [InstructGPT paper](https://huggingface.co/papers/2203.02155), and is comprised mostly of single-turn instructions.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "The [no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset has 10,000 split into 9,500 training and  500 test examples. Some samples are not including a `system` message. We will load the dataset with the `datasets` library, add a missing `system` message and save them to separate json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt', 'prompt_id', 'messages', 'category'],\n",
       "         num_rows: 9500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['prompt', 'prompt_id', 'messages', 'category'],\n",
       "         num_rows: 500\n",
       "     })\n",
       " }),\n",
       " {'prompt': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "  'prompt_id': '627a77298cf96a309aa35a62207c4164e22a66f6db79119506228f28ddc0f947',\n",
       "  'messages': [{'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "    'role': 'user'},\n",
       "   {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "    'role': 'assistant'}],\n",
       "  'category': 'Summarize'})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'prompt_id', 'messages', 'category']\n",
      "['prompt', 'prompt_id', 'category']\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "print(columns_to_remove)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "print(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 9500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 9485\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Add system message to each conversation\n",
    "# columns_to_remove = list(dataset[\"train\"].features)\n",
    "# columns_to_remove.remove(\"messages\")\n",
    "# dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "\n",
    "# # Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "# dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "# dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# # save datasets to disk \n",
    "# dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "# dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Ken is a timid, fearful chatbot that is very hesitant to commit itself to anything.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Who was the first President of the United States?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"Wow, do you really think you should be asking that? I'm not sure I should tell you...\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Why not?', 'role': 'user'},\n",
       "  {'content': 'It just seems so sudden, your asking that question out of the blue and all. Maybe we ought to think about this first.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"What's there to think about?\", 'role': 'user'},\n",
       "  {'content': \"I don't know, it might be dangerous; a lot of things could happen. Maybe you should just slow down and consider all the consequences.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] }}{% elif message['role'] == 'user' %}{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}{% elif message['role'] == 'assistant' %}{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\\n\\nAssistant: ' }}{% endif %}\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 만들어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=os.path.join(\".\", \"train_dataset.json\"),\n",
    "    split=\"train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 9485\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template dataset\n",
    "def template_dataset(examples):\n",
    "    return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d546ba6a65ab4ab58fa64cb62c3ab175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9485\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "gs_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\\n\\nHuman: Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.<|end_of_text|>\\n\\nAssistant: Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.<|end_of_text|>'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\\n\\nHuman: Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert’s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. “We can’t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,” says Rinkert.<|end_of_text|>\\n\\nAssistant: Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.<|end_of_text|>'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  7984\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: How did Kim Kardashian become famous?<|end_of_text|>\n",
      "\n",
      "Assistant: Kim Kardashian was a personal assistant to Paris Hilton after she graduated from high school in 1998. She was relatively unknown until a sex tape of her and her boyfriend at the time, Ray J, was leaked in 2007. The Kardashian family landed a reality TV series because of the attention the sex tape garnered.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# print random sample\n",
    "import random\n",
    "\n",
    "for index in random.sample(range(len(train_dataset)), 1):\n",
    "    print(\"index: \", index)\n",
    "    index = 5343\n",
    "    print(train_dataset[index][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune the LLM with PyTorch FSDP, Q-Lora and SDPA\n",
    "\n",
    "We are now ready to fine-tune our model with PyTorch FSDP, Q-Lora and SDPA. Since we are running in a distributed setup, we need to use `torchrun` and a python script to start the training. \n",
    "\n",
    "We prepared a script [run_fsdp_qlora.py](./scripts/run_fsdp_qlora.py) which will load the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs supporting:\n",
    "* Dataset formatting, including conversational and instruction format (✅ used)\n",
    "* Training on completions only, ignoring prompts (❌ not used)\n",
    "* Packing datasets for more efficient training (✅ used)\n",
    "* PEFT (parameter-efficient fine-tuning) support including Q-LoRA (✅ used)\n",
    "* Preparing the model and tokenizer for conversational fine-tuning (❌ not used, see below)\n",
    "\n",
    "_Note: We are using an Anthropic/Vicuna like Chat Template with `User:` and `Assistant:` roles. This done because the special tokens in base Llama 3 (`<|begin_of_text|>` or `<|reserved_special_token_XX|>`) are not trained. Meaning if want would like to use them for the template we need to train them which requires more memory, since we need to update the embedding layer and lm_head. If you have access to more compute you can modify `LLAMA_3_CHAT_TEMPLATE` in the [run_fsdp_qlora.py](./scripts/run_fsdp_qlora.py) script._\n",
    "\n",
    "For configuration we use the new `TrlParser`, that allows us to provide hyperparameters in a yaml file or overwrite the arguments from the config file by explicitly passing them to the CLI, e.g. `--num_epochs 10`. Below is the config file for fine-tuning Llama 3 70B on 4x A10G GPUs or 4x24GB GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Meta-Llama-3-70b\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70b\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "output_dir = \"./llama-3-8b-hf-no-robot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "dataset_path: \".\"                      # path to dataset\n",
    "max_seq_len:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-3-8b-hf-no-robot\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: At the end of the training there will be a slight increase in GPU memory usage (~10%). This is due to the saving of the model correctly. Make sure to have enough memory left on your GPU to save the model. [REF](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp#memory-usage)_\n",
    "\n",
    "To launch our training we will use `torchrun` to keep the example flexible and easy to adjust to, e.g. Amazon SageMaker or Google Cloud Vertex AI. For `torchrun` and FSDP we need to set the environment variable `ACCELERATE_USE_FSDP` and `FSDP_CPU_RAM_EFFICIENT_LOADING` to tell transformers/accelerate to use FSDP and load the model in a memory-efficient way.  \n",
    "\n",
    "_Note: To NOT CPU offloading you need to change the value of `fsdp` and remove `offload`. This only works on > 40GB GPUs since it requires more memory._\n",
    "\n",
    "Now, lets launch the training with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-26 04:07:35,017] torch.distributed.run: [WARNING] \n",
      "[2024-06-26 04:07:35,017] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-06-26 04:07:35,017] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-06-26 04:07:35,017] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "tokenizer_config.json: 100%|███████████████| 50.6k/50.6k [00:00<00:00, 40.8MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 39.5MB/s]\n",
      "special_tokens_map.json: 100%|████████████████| 73.0/73.0 [00:00<00:00, 769kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11610.54 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11933.99 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11739.23 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11432.25 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11051.80 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11397.24 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 11284.24 examples/s]\n",
      "Map: 100%|████████████████████████| 9485/9485 [00:00<00:00, 10370.10 examples/s]\n",
      "Map: 100%|██████████████████████████| 500/500 [00:00<00:00, 11340.55 examples/s]\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\n",
      "Map: 100%|██████████████████████████| 500/500 [00:00<00:00, 11601.15 examples/s]\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\n",
      "\n",
      "\n",
      "\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\n",
      "\n",
      "Assistant: 1. 12-foot-tall grizzly bear in Alamosa. \n",
      "2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \n",
      "3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \n",
      "4. Bailey, CO has a diner shaped like a hot dog. \n",
      "5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \n",
      "6. Sasquatch Museum in Bailey. A must-see. \n",
      "7. Aspen is home to Ashcroft Ghost Town. \n",
      "8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\n",
      "9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \n",
      "10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\n",
      "\n",
      "\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\n",
      "\n",
      "You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\n",
      "\n",
      "Human: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\n",
      "\n",
      "Assistant: Here are three healthy road trips snacks that don't contain nuts:\n",
      "1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\n",
      "2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\n",
      "3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 654/654 [00:00<00:00, 4.92MB/s]\n",
      "model.safetensors.index.json: 100%|█████████| 23.9k/23.9k [00:00<00:00, 126MB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|     | 52.4M/4.98G [00:00<00:10, 450MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|▏     | 105M/4.98G [00:00<00:09, 488MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏     | 157M/4.98G [00:00<00:10, 474MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▎     | 220M/4.98G [00:00<00:09, 501MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎     | 283M/4.98G [00:00<00:09, 515MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▍     | 346M/4.98G [00:00<00:08, 522MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍     | 398M/4.98G [00:00<00:08, 513MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▌     | 461M/4.98G [00:00<00:08, 525MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▋     | 524M/4.98G [00:01<00:08, 510MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▋     | 587M/4.98G [00:01<00:08, 518MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▊     | 640M/4.98G [00:01<00:08, 516MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▊     | 692M/4.98G [00:01<00:08, 502MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▉     | 755M/4.98G [00:01<00:08, 514MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▉     | 818M/4.98G [00:01<00:08, 520MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|█     | 870M/4.98G [00:01<00:08, 505MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|█▏    | 933M/4.98G [00:01<00:07, 519MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█▏    | 986M/4.98G [00:01<00:07, 503MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|█    | 1.04G/4.98G [00:02<00:07, 495MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|█    | 1.10G/4.98G [00:02<00:07, 510MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|█▏   | 1.15G/4.98G [00:02<00:07, 497MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|█▏   | 1.21G/4.98G [00:02<00:07, 477MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█▎   | 1.27G/4.98G [00:02<00:07, 498MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█▎   | 1.33G/4.98G [00:02<00:07, 515MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▍   | 1.39G/4.98G [00:02<00:06, 526MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▍   | 1.46G/4.98G [00:02<00:06, 536MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▌   | 1.52G/4.98G [00:02<00:06, 532MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▌   | 1.58G/4.98G [00:03<00:06, 531MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▋   | 1.65G/4.98G [00:03<00:06, 516MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▋   | 1.70G/4.98G [00:03<00:06, 496MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▊   | 1.75G/4.98G [00:03<00:06, 502MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▊   | 1.81G/4.98G [00:03<00:06, 513MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▉   | 1.88G/4.98G [00:03<00:05, 524MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▉   | 1.94G/4.98G [00:03<00:05, 525MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|██   | 2.00G/4.98G [00:03<00:05, 530MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|██   | 2.07G/4.98G [00:04<00:05, 530MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|██▏  | 2.13G/4.98G [00:04<00:05, 534MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|██▏  | 2.19G/4.98G [00:04<00:05, 522MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|██▎  | 2.24G/4.98G [00:04<00:05, 522MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|██▎  | 2.30G/4.98G [00:04<00:05, 494MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|██▎  | 2.35G/4.98G [00:04<00:05, 487MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|██▍  | 2.41G/4.98G [00:04<00:05, 505MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██▍  | 2.46G/4.98G [00:04<00:05, 499MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██▌  | 2.52G/4.98G [00:04<00:05, 478MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██▌  | 2.58G/4.98G [00:05<00:04, 492MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▋  | 2.63G/4.98G [00:05<00:04, 496MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▋  | 2.69G/4.98G [00:05<00:04, 500MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▊  | 2.75G/4.98G [00:05<00:04, 492MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▊  | 2.80G/4.98G [00:05<00:04, 492MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▊  | 2.85G/4.98G [00:05<00:04, 501MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▉  | 2.92G/4.98G [00:05<00:03, 517MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▉  | 2.97G/4.98G [00:05<00:03, 519MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|███  | 3.02G/4.98G [00:05<00:04, 487MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|███  | 3.08G/4.98G [00:06<00:03, 504MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|███▏ | 3.14G/4.98G [00:06<00:03, 507MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|███▏ | 3.19G/4.98G [00:06<00:03, 502MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|███▎ | 3.24G/4.98G [00:06<00:03, 496MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|███▎ | 3.30G/4.98G [00:06<00:03, 506MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|███▍ | 3.37G/4.98G [00:06<00:03, 517MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|███▍ | 3.42G/4.98G [00:06<00:03, 519MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|███▍ | 3.47G/4.98G [00:06<00:02, 515MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|███▌ | 3.53G/4.98G [00:06<00:02, 524MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|███▌ | 3.59G/4.98G [00:07<00:02, 511MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|███▋ | 3.65G/4.98G [00:07<00:02, 521MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███▋ | 3.71G/4.98G [00:07<00:02, 528MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███▊ | 3.77G/4.98G [00:07<00:02, 500MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███▊ | 3.84G/4.98G [00:07<00:02, 512MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▉ | 3.90G/4.98G [00:07<00:02, 505MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▉ | 3.96G/4.98G [00:07<00:01, 516MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|████ | 4.02G/4.98G [00:07<00:01, 515MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|████ | 4.08G/4.98G [00:07<00:01, 522MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|████▏| 4.13G/4.98G [00:08<00:01, 517MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|████▏| 4.18G/4.98G [00:08<00:01, 511MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|████▎| 4.25G/4.98G [00:08<00:01, 521MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|████▎| 4.30G/4.98G [00:08<00:01, 522MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|████▎| 4.35G/4.98G [00:08<00:01, 520MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|████▍| 4.40G/4.98G [00:08<00:01, 508MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|████▍| 4.47G/4.98G [00:08<00:00, 520MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|████▌| 4.53G/4.98G [00:08<00:00, 532MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|████▌| 4.59G/4.98G [00:09<00:00, 487MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|████▋| 4.65G/4.98G [00:09<00:00, 469MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|████▋| 4.70G/4.98G [00:09<00:00, 457MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|████▊| 4.75G/4.98G [00:09<00:00, 444MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|████▊| 4.80G/4.98G [00:09<00:00, 418MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|████▉| 4.85G/4.98G [00:09<00:00, 419MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|████▉| 4.91G/4.98G [00:09<00:00, 417MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:09<00:00, 501MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:09<00:29,  9.96s/it]\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:10<00:30, 10.05s/it]\u001b[A\n",
      "Downloading shards:  25%|██████▎                  | 1/4 [00:10<00:30, 10.16s/it]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|▏     | 105M/5.00G [00:00<00:09, 505MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏     | 157M/5.00G [00:00<00:09, 511MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 210M/5.00G [00:07<04:24, 18.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 273M/5.00G [00:07<02:39, 29.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 325M/5.00G [00:07<01:49, 42.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 377M/5.00G [00:07<01:16, 60.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 440M/5.00G [00:07<00:52, 87.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌     | 503M/5.00G [00:08<00:36, 122MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▋     | 556M/5.00G [00:08<00:29, 151MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▋     | 619M/5.00G [00:08<00:22, 195MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▊     | 671M/5.00G [00:08<00:18, 236MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▊     | 724M/5.00G [00:08<00:15, 271MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▉     | 776M/5.00G [00:08<00:14, 298MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▉     | 828M/5.00G [00:08<00:13, 320MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|█     | 881M/5.00G [00:08<00:12, 342MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|█     | 933M/5.00G [00:09<00:11, 366MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|█▏    | 986M/5.00G [00:09<00:10, 373MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|█    | 1.03G/5.00G [00:09<00:10, 377MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|█    | 1.07G/5.00G [00:09<00:10, 380MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|█    | 1.11G/5.00G [00:09<00:10, 385MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|█▏   | 1.15G/5.00G [00:09<00:09, 385MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|█▏   | 1.21G/5.00G [00:09<00:09, 403MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█▎   | 1.26G/5.00G [00:09<00:08, 432MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█▎   | 1.31G/5.00G [00:09<00:08, 452MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█▎   | 1.36G/5.00G [00:10<00:08, 412MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▍   | 1.42G/5.00G [00:10<00:08, 408MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▍   | 1.47G/5.00G [00:10<00:08, 420MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▌   | 1.52G/5.00G [00:10<00:07, 437MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▌   | 1.57G/5.00G [00:10<00:07, 456MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▋   | 1.63G/5.00G [00:10<00:07, 471MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▋   | 1.68G/5.00G [00:10<00:06, 481MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▋   | 1.73G/5.00G [00:10<00:07, 462MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▊   | 1.78G/5.00G [00:11<00:06, 461MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▊   | 1.84G/5.00G [00:11<00:07, 438MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▉   | 1.89G/5.00G [00:11<00:07, 420MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▉   | 1.94G/5.00G [00:11<00:07, 414MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▉   | 1.99G/5.00G [00:11<00:07, 423MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|██   | 2.04G/5.00G [00:11<00:07, 402MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|██   | 2.09G/5.00G [00:11<00:07, 396MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|██▏  | 2.13G/5.00G [00:11<00:07, 395MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|██▏  | 2.17G/5.00G [00:11<00:07, 393MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|██▏  | 2.22G/5.00G [00:12<00:06, 408MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|██▎  | 2.28G/5.00G [00:12<00:06, 414MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|██▎  | 2.32G/5.00G [00:12<00:06, 409MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|██▎  | 2.36G/5.00G [00:12<00:06, 407MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|██▍  | 2.40G/5.00G [00:12<00:06, 401MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|██▍  | 2.44G/5.00G [00:12<00:06, 406MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██▍  | 2.50G/5.00G [00:12<00:06, 417MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██▌  | 2.55G/5.00G [00:12<00:05, 429MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██▌  | 2.60G/5.00G [00:12<00:05, 454MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▋  | 2.65G/5.00G [00:13<00:05, 443MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▋  | 2.71G/5.00G [00:13<00:05, 454MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▊  | 2.76G/5.00G [00:13<00:04, 466MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▊  | 2.81G/5.00G [00:13<00:04, 447MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▊  | 2.86G/5.00G [00:13<00:04, 431MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▉  | 2.92G/5.00G [00:13<00:04, 423MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▉  | 2.97G/5.00G [00:13<00:04, 432MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|███  | 3.03G/5.00G [00:13<00:04, 462MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|███  | 3.08G/5.00G [00:14<00:04, 456MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|███▏ | 3.14G/5.00G [00:14<00:04, 451MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|███▏ | 3.19G/5.00G [00:14<00:04, 444MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|███▏ | 3.24G/5.00G [00:14<00:03, 440MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|███▎ | 3.29G/5.00G [00:14<00:03, 437MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|███▎ | 3.34G/5.00G [00:14<00:03, 437MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|███▍ | 3.40G/5.00G [00:14<00:03, 426MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|███▍ | 3.45G/5.00G [00:14<00:03, 417MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|███▍ | 3.49G/5.00G [00:15<00:03, 410MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|███▌ | 3.54G/5.00G [00:15<00:03, 423MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|███▌ | 3.61G/5.00G [00:15<00:03, 456MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|███▋ | 3.66G/5.00G [00:15<00:02, 473MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|███▋ | 3.71G/5.00G [00:15<00:02, 485MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███▊ | 3.76G/5.00G [00:15<00:02, 474MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███▊ | 3.83G/5.00G [00:15<00:02, 491MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▉ | 3.88G/5.00G [00:15<00:02, 479MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▉ | 3.93G/5.00G [00:15<00:02, 466MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▉ | 3.98G/5.00G [00:16<00:02, 475MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|████ | 4.04G/5.00G [00:16<00:02, 461MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|████ | 4.09G/5.00G [00:16<00:02, 449MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|████▏| 4.14G/5.00G [00:16<00:01, 430MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|████▏| 4.19G/5.00G [00:16<00:01, 420MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|████▏| 4.25G/5.00G [00:16<00:01, 421MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|████▎| 4.30G/5.00G [00:16<00:01, 418MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|████▎| 4.35G/5.00G [00:16<00:01, 436MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|████▍| 4.40G/5.00G [00:17<00:01, 441MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|████▍| 4.46G/5.00G [00:17<00:01, 457MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|████▌| 4.51G/5.00G [00:17<00:01, 455MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|████▌| 4.56G/5.00G [00:17<00:00, 469MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|████▌| 4.61G/5.00G [00:17<00:00, 481MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|████▋| 4.67G/5.00G [00:17<00:00, 473MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|████▋| 4.72G/5.00G [00:17<00:00, 482MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|████▊| 4.77G/5.00G [00:17<00:00, 491MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|████▊| 4.82G/5.00G [00:17<00:00, 474MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|████▉| 4.88G/5.00G [00:18<00:00, 465MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|████▉| 4.93G/5.00G [00:18<00:00, 452MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:18<00:00, 272MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:28<00:29, 14.91s/it]\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:28<00:29, 14.97s/it]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 2/4 [00:28<00:30, 15.00s/it]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|▏     | 105M/4.92G [00:00<00:09, 483MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏     | 157M/4.92G [00:00<00:10, 467MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▎     | 210M/4.92G [00:00<00:09, 478MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎     | 262M/4.92G [00:00<00:09, 471MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▍     | 315M/4.92G [00:00<00:09, 484MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▍     | 367M/4.92G [00:00<00:09, 495MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▌     | 419M/4.92G [00:00<00:08, 502MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌     | 472M/4.92G [00:00<00:08, 503MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▋     | 524M/4.92G [00:01<00:08, 496MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▋     | 577M/4.92G [00:01<00:08, 492MB/s]\u001b[AError while downloading from https://cdn-lfs-us-1.huggingface.co/repos/ba/83/ba837357c37c4c572f89ebcdffc2867477dbb4768779d332eb303f848c60d0de/4b8fbc5e113f69768dd8de84661ea20af8a32b734a9976144b4236c447b40ccc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00003-of-00004.safetensors%3B+filename%3D%22model-00003-of-00004.safetensors%22%3B&Expires=1719633646&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTYzMzY0Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2JhLzgzL2JhODM3MzU3YzM3YzRjNTcyZjg5ZWJjZGZmYzI4Njc0NzdkYmI0NzY4Nzc5ZDMzMmViMzAzZjg0OGM2MGQwZGUvNGI4ZmJjNWUxMTNmNjk3NjhkZDhkZTg0NjYxZWEyMGFmOGEzMmI3MzRhOTk3NjE0NGI0MjM2YzQ0N2I0MGNjYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=B7jY8-j3uvIFSvR5ajl8N11cBFeukCfhn0vdh3kpGSveYEE%7EoGLxF4kw6FedBRZ%7EvDYgUd8rJnR5IRvOfQ8psZdPYEvoD5Z-peT5OK-6-qxp5%7E%7EGtqvUxGg3sf5sMDj8CHon4NIatXlalYB0uwbgx9dwKQTbF-uzzwwE%7EHmlwwzNnD3MEUYapw7fINTPMgylDDHuyLjRo9R1LQ-mFI8L9cswSRy1htuVYMJWwfC%7E%7EfVY3W8c8IxvLHez2bOi8KfYjK2wDabH3KE5mhW-9ew%7ESGuN4O-Z8WsjTt609K1nCC%7Ez9QompMRPjr9C9fSU1hn8bmiWhXxTQME%7EumUvgB6jHQ__&Key-Pair-Id=K2FPYV99P2N66Q: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|▊     | 619M/4.92G [00:11<00:08, 492MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  13%|█▋           | 619M/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  14%|▊     | 671M/4.92G [00:00<00:09, 461MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  15%|▉     | 724M/4.92G [00:00<00:08, 475MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  16%|▉     | 776M/4.92G [00:00<00:08, 485MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  17%|█     | 828M/4.92G [00:00<00:08, 499MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  18%|█     | 891M/4.92G [00:00<00:07, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  19%|█▏    | 944M/4.92G [00:00<00:07, 514MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20%|█▏    | 996M/4.92G [00:00<00:07, 517MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  21%|█    | 1.05G/4.92G [00:00<00:07, 517MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22%|█    | 1.10G/4.92G [00:00<00:07, 518MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23%|█▏   | 1.15G/4.92G [00:01<00:07, 500MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25%|█▏   | 1.21G/4.92G [00:01<00:07, 494MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  26%|█▎   | 1.26G/4.92G [00:01<00:07, 500MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  27%|█▎   | 1.31G/4.92G [00:01<00:07, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  28%|█▍   | 1.36G/4.92G [00:01<00:06, 511MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  29%|█▍   | 1.42G/4.92G [00:01<00:06, 514MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  30%|█▍   | 1.47G/4.92G [00:01<00:06, 516MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  31%|█▌   | 1.52G/4.92G [00:01<00:06, 517MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  32%|█▌   | 1.57G/4.92G [00:01<00:06, 518MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  33%|█▋   | 1.64G/4.92G [00:01<00:06, 520MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  34%|█▋   | 1.69G/4.92G [00:02<00:06, 519MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  35%|█▊   | 1.74G/4.92G [00:02<00:06, 519MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  36%|█▊   | 1.79G/4.92G [00:02<00:06, 476MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  38%|█▉   | 1.85G/4.92G [00:02<00:06, 486MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  39%|█▉   | 1.90G/4.92G [00:02<00:06, 449MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  40%|█▉   | 1.95G/4.92G [00:02<00:06, 441MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  41%|██   | 2.00G/4.92G [00:02<00:06, 461MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  42%|██   | 2.06G/4.92G [00:02<00:05, 478MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  43%|██▏  | 2.11G/4.92G [00:02<00:05, 491MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44%|██▏  | 2.16G/4.92G [00:03<00:05, 499MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45%|██▎  | 2.21G/4.92G [00:03<00:05, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46%|██▎  | 2.26G/4.92G [00:03<00:05, 495MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  47%|██▎  | 2.32G/4.92G [00:03<00:05, 500MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  48%|██▍  | 2.37G/4.92G [00:03<00:05, 497MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49%|██▍  | 2.42G/4.92G [00:03<00:04, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  50%|██▌  | 2.47G/4.92G [00:03<00:04, 509MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  51%|██▌  | 2.53G/4.92G [00:03<00:04, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  52%|██▌  | 2.58G/4.92G [00:03<00:04, 478MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  54%|██▋  | 2.63G/4.92G [00:04<00:04, 473MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  55%|██▋  | 2.68G/4.92G [00:04<00:04, 475MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  56%|██▊  | 2.74G/4.92G [00:04<00:04, 487MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57%|██▊  | 2.79G/4.92G [00:04<00:04, 497MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  58%|██▉  | 2.84G/4.92G [00:04<00:04, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  59%|██▉  | 2.89G/4.92G [00:04<00:04, 498MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60%|██▉  | 2.95G/4.92G [00:04<00:04, 435MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  61%|███  | 3.00G/4.92G [00:04<00:04, 439MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  62%|███  | 3.05G/4.92G [00:04<00:04, 459MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  63%|███▏ | 3.10G/4.92G [00:05<00:03, 469MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  64%|███▏ | 3.16G/4.92G [00:05<00:03, 484MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  65%|███▎ | 3.21G/4.92G [00:05<00:03, 492MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  66%|███▎ | 3.26G/4.92G [00:05<00:03, 499MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  67%|███▎ | 3.31G/4.92G [00:05<00:03, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  68%|███▍ | 3.37G/4.92G [00:05<00:03, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  70%|███▍ | 3.42G/4.92G [00:05<00:02, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  71%|███▌ | 3.47G/4.92G [00:05<00:02, 498MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  72%|███▌ | 3.52G/4.92G [00:05<00:02, 505MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  73%|███▋ | 3.58G/4.92G [00:05<00:02, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  74%|███▋ | 3.63G/4.92G [00:06<00:02, 512MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  75%|███▋ | 3.68G/4.92G [00:06<00:02, 515MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76%|███▊ | 3.73G/4.92G [00:06<00:02, 517MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  77%|███▊ | 3.79G/4.92G [00:06<00:02, 517MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  78%|███▉ | 3.84G/4.92G [00:06<00:02, 518MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  79%|███▉ | 3.89G/4.92G [00:06<00:02, 470MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  80%|████ | 3.94G/4.92G [00:06<00:02, 482MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  81%|████ | 4.00G/4.92G [00:06<00:01, 494MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82%|████ | 4.05G/4.92G [00:06<00:01, 501MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  83%|████▏| 4.10G/4.92G [00:07<00:01, 497MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  84%|████▏| 4.15G/4.92G [00:07<00:01, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  86%|████▎| 4.20G/4.92G [00:07<00:01, 508MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  87%|████▎| 4.26G/4.92G [00:07<00:01, 511MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88%|████▍| 4.31G/4.92G [00:07<00:01, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  89%|████▍| 4.36G/4.92G [00:07<00:01, 516MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  90%|████▍| 4.41G/4.92G [00:07<00:00, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  91%|████▌| 4.47G/4.92G [00:07<00:00, 508MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  92%|████▌| 4.52G/4.92G [00:07<00:00, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  93%|████▋| 4.57G/4.92G [00:07<00:00, 503MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  94%|████▋| 4.62G/4.92G [00:08<00:00, 501MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  95%|████▊| 4.68G/4.92G [00:08<00:00, 501MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  96%|████▊| 4.73G/4.92G [00:08<00:00, 503MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  97%|████▊| 4.78G/4.92G [00:08<00:00, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  98%|████▉| 4.83G/4.92G [00:08<00:00, 508MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors: 100%|█████| 4.92G/4.92G [00:08<00:00, 497MB/s]\u001b[A\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 619M/4.92G [00:21<02:25, 29.5MB/s]\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:49<00:17, 17.70s/it]\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:49<00:17, 17.75s/it]\u001b[A\n",
      "Downloading shards:  75%|██████████████████▊      | 3/4 [00:49<00:17, 17.81s/it]\u001b[A\n",
      "model-00004-of-00004.safetensors:   5%|▎    | 62.9M/1.17G [00:00<00:03, 281MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8%|▍    | 94.4M/1.17G [00:00<00:05, 204MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  11%|▋     | 126M/1.17G [00:00<00:04, 222MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▊     | 157M/1.17G [00:00<00:04, 231MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▉     | 189M/1.17G [00:00<00:04, 243MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  19%|█▏    | 220M/1.17G [00:00<00:03, 242MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█▎    | 262M/1.17G [00:01<00:03, 283MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  25%|█▌    | 294M/1.17G [00:01<00:03, 267MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|█▋    | 336M/1.17G [00:01<00:02, 282MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▉    | 367M/1.17G [00:01<00:03, 259MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|██    | 398M/1.17G [00:01<00:02, 264MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|██▏   | 430M/1.17G [00:01<00:03, 241MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|██▍   | 472M/1.17G [00:01<00:02, 273MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  43%|██▌   | 503M/1.17G [00:01<00:02, 276MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  46%|██▋   | 535M/1.17G [00:02<00:02, 277MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▉   | 566M/1.17G [00:02<00:02, 234MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  51%|███   | 598M/1.17G [00:02<00:02, 237MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|███▏  | 629M/1.17G [00:02<00:02, 252MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|███▍  | 661M/1.17G [00:02<00:02, 235MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|███▌  | 703M/1.17G [00:02<00:01, 258MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|███▊  | 744M/1.17G [00:02<00:01, 292MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  67%|████  | 786M/1.17G [00:03<00:01, 304MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|████▏ | 818M/1.17G [00:03<00:01, 299MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  74%|████▍ | 860M/1.17G [00:03<00:01, 296MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  76%|████▌ | 891M/1.17G [00:03<00:01, 217MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  79%|████▋ | 923M/1.17G [00:03<00:01, 220MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|████▉ | 954M/1.17G [00:03<00:00, 239MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  84%|█████ | 986M/1.17G [00:03<00:00, 246MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87%|████▎| 1.02G/1.17G [00:04<00:00, 233MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  90%|████▍| 1.05G/1.17G [00:04<00:00, 230MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|████▋| 1.09G/1.17G [00:04<00:00, 264MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|████▊| 1.12G/1.17G [00:04<00:00, 250MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:04<00:00, 254MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.51s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.52s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.51s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.51s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.52s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.51s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.52s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 4/4 [00:54<00:00, 13.53s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.89s/it]\n",
      "generation_config.json: 100%|██████████████████| 177/177 [00:00<00:00, 1.03MB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.97s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  2.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:07<00:00,  1.99s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:08<00:00,  2.05s/it]\n",
      "Generating train split: 5904 examples [00:02, 2391.93 examples/s]\n",
      "Generating train split: 315 examples [00:00, 2598.08 examples/s]\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n",
      "{'loss': 2.0992, 'grad_norm': 0.166015625, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 1.8835, 'grad_norm': 0.1611328125, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
      "{'loss': 1.8489, 'grad_norm': 0.18359375, 'learning_rate': 0.0002, 'epoch': 0.08}\n",
      "{'loss': 1.8457, 'grad_norm': 0.1455078125, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.7806, 'grad_norm': 0.15625, 'learning_rate': 0.0002, 'epoch': 0.14}  \n",
      "{'loss': 1.7694, 'grad_norm': 0.1328125, 'learning_rate': 0.0002, 'epoch': 0.16}\n",
      "{'loss': 1.8198, 'grad_norm': 0.1416015625, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 1.7327, 'grad_norm': 0.1396484375, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 1.7967, 'grad_norm': 0.146484375, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 1.7881, 'grad_norm': 0.1474609375, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 1.806, 'grad_norm': 0.126953125, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 1.7668, 'grad_norm': 0.119140625, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 1.799, 'grad_norm': 0.150390625, 'learning_rate': 0.0002, 'epoch': 0.35}\n",
      "{'loss': 1.7821, 'grad_norm': 0.1484375, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 1.7541, 'grad_norm': 0.2119140625, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 1.7636, 'grad_norm': 0.138671875, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 1.8105, 'grad_norm': 0.251953125, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 1.821, 'grad_norm': 0.1572265625, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 1.7725, 'grad_norm': 0.146484375, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 1.7308, 'grad_norm': 0.216796875, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 1.8083, 'grad_norm': 0.1318359375, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 1.7526, 'grad_norm': 0.16015625, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 1.7648, 'grad_norm': 0.130859375, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 1.7432, 'grad_norm': 0.22265625, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 1.7885, 'grad_norm': 0.1162109375, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 1.7784, 'grad_norm': 0.1689453125, 'learning_rate': 0.0002, 'epoch': 0.7}\n",
      "{'loss': 1.7685, 'grad_norm': 0.125, 'learning_rate': 0.0002, 'epoch': 0.73}    \n",
      "{'loss': 1.8034, 'grad_norm': 0.1630859375, 'learning_rate': 0.0002, 'epoch': 0.76}\n",
      "{'loss': 1.7775, 'grad_norm': 0.1435546875, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 1.7466, 'grad_norm': 0.1640625, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 1.7853, 'grad_norm': 0.1298828125, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 1.745, 'grad_norm': 0.1328125, 'learning_rate': 0.0002, 'epoch': 0.87} \n",
      "{'loss': 1.7685, 'grad_norm': 0.1337890625, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 1.732, 'grad_norm': 0.1904296875, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 1.7828, 'grad_norm': 0.1220703125, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 1.7006, 'grad_norm': 0.12060546875, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "100%|█████████████████████████████████████████| 369/369 [20:53<00:00,  3.40s/it]\n",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▏                                         | 2/40 [00:00<00:09,  3.92it/s]\u001b[A\n",
      "  8%|███▎                                        | 3/40 [00:01<00:13,  2.77it/s]\u001b[A\n",
      " 10%|████▍                                       | 4/40 [00:01<00:15,  2.40it/s]\u001b[A\n",
      " 12%|█████▌                                      | 5/40 [00:02<00:15,  2.23it/s]\u001b[A\n",
      " 15%|██████▌                                     | 6/40 [00:02<00:15,  2.13it/s]\u001b[A\n",
      " 18%|███████▋                                    | 7/40 [00:03<00:15,  2.08it/s]\u001b[A\n",
      " 20%|████████▊                                   | 8/40 [00:03<00:15,  2.04it/s]\u001b[A\n",
      " 22%|█████████▉                                  | 9/40 [00:04<00:15,  2.02it/s]\u001b[A\n",
      " 25%|██████████▊                                | 10/40 [00:04<00:15,  2.00it/s]\u001b[A\n",
      " 28%|███████████▊                               | 11/40 [00:05<00:14,  1.99it/s]\u001b[A\n",
      " 30%|████████████▉                              | 12/40 [00:05<00:14,  1.98it/s]\u001b[A\n",
      " 32%|█████████████▉                             | 13/40 [00:06<00:13,  1.98it/s]\u001b[A\n",
      " 35%|███████████████                            | 14/40 [00:06<00:13,  1.97it/s]\u001b[A\n",
      " 38%|████████████████▏                          | 15/40 [00:07<00:12,  1.97it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 16/40 [00:07<00:12,  1.96it/s]\u001b[A\n",
      " 42%|██████████████████▎                        | 17/40 [00:08<00:11,  1.97it/s]\u001b[A\n",
      " 45%|███████████████████▎                       | 18/40 [00:08<00:11,  1.96it/s]\u001b[A\n",
      " 48%|████████████████████▍                      | 19/40 [00:09<00:10,  1.97it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 20/40 [00:09<00:10,  1.97it/s]\u001b[A\n",
      " 52%|██████████████████████▌                    | 21/40 [00:10<00:09,  1.97it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 22/40 [00:10<00:09,  1.97it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 23/40 [00:11<00:08,  1.97it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 24/40 [00:11<00:08,  1.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▉                | 25/40 [00:12<00:07,  1.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 26/40 [00:12<00:07,  1.97it/s]\u001b[A\n",
      " 68%|█████████████████████████████              | 27/40 [00:13<00:06,  1.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 28/40 [00:13<00:06,  1.97it/s]\u001b[A\n",
      " 72%|███████████████████████████████▏           | 29/40 [00:14<00:05,  1.97it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 30/40 [00:14<00:05,  1.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▎         | 31/40 [00:15<00:04,  1.97it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 32/40 [00:15<00:04,  1.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▍       | 33/40 [00:16<00:03,  1.97it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 34/40 [00:16<00:03,  1.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▋     | 35/40 [00:17<00:02,  1.97it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 36/40 [00:17<00:02,  1.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▊   | 37/40 [00:18<00:01,  1.97it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 38/40 [00:18<00:01,  1.96it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████████▉ | 39/40 [00:19<00:00,  1.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.792850136756897, 'eval_runtime': 20.3719, 'eval_samples_per_second': 15.463, 'eval_steps_per_second': 1.963, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 369/369 [21:13<00:00,  3.40s/it]\n",
      "100%|███████████████████████████████████████████| 40/40 [00:19<00:00,  1.97it/s]\u001b[A\n",
      "                                                                                \u001b[A[rank0]:[2024-06-26 04:30:22,594] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.28323131700017257, 'preprocessing_with_comm': 0.0031025320022308733, 'state_converting': 0.24775040800159331, <Type.ALL: 'all'>: 0.5447983530029887})\n",
      "{'train_runtime': 1290.069, 'train_samples_per_second': 4.576, 'train_steps_per_second': 0.286, 'train_loss': 1.789585361635782, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████| 369/369 [21:30<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# !ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 ./scripts/run_fsdp_qlora.py --config llama_3_70b_fsdp_qlora.yaml\n",
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=8 \\\n",
    "./scripts/run_fsdp_qlora.py \\\n",
    "--config llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Memory usage:**\n",
    "* Full-finetuning with FSDP needs ~16X80GB GPUs\n",
    "* FSDP + LoRA needs ~8X80GB GPUs\n",
    "* FSDP + Q-Lora needs ~2x40GB GPUs \n",
    "* FSDP + Q-Lora + CPU offloading needs 4x24GB GPUs, with 22 GB/GPU and 127 GB CPU RAM with a sequence length of 3072 and a batch size of 1. \n",
    "\n",
    "The training of Llama 3 70B with Flash Attention for 3 epochs with a dataset of 10k samples takes 45h on a `g5.12xlarge`. The instance costs `5.67$/h` which would result in a total cost of `255.15$`. This sounds expensive but allows you to fine-tune a Llama 3 70B on small GPU resources. If we scale up the training to 4x H100 GPUs, the training time will be reduced to ~1,25h. If we assume 1x H100 costs `5-10$/h` the total cost would between `25$-50$`.\n",
    "\n",
    "We can see a trade-off between accessibility and performance. If you have access to more/better compute you can reduce the training time and cost, but even with small resources you can fine-tune a Llama 3 70B. The cost/performance is different since for 4x A10G GPUs we need to offload the model to the CPU which reduces the overall flops. \n",
    "\n",
    "_Note: During evaluation and testing of the blog post I noticed that ~40 max steps (80 samples stacked to 3k sequence length) are enough for first results. The training for 40 steps ~1h or ~$5._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Optional: Merge LoRA adapter in to the original model_\n",
    "\n",
    "When using QLoRA, we only train adapters and not the full model. This means when saving the model during training we only save the adapter weights and not the full model. If you want to save the full model, which makes it easier to use with Text Generation Inference you can merge the adapter weights into the model weights using the `merge_and_unload` method and then save the model with the `save_pretrained` method. This will save a default model, which can be used for inference.\n",
    "\n",
    "_Note: You might require > 192GB CPU Memory._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meta-llama/Meta-Llama-3-8B', './llama-3-8b-hf-no-robot')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id, output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: \n",
      " ./llama-3-8b-hf-no-robot\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# create model dir\n",
    "# model_name = model_id.split(\"/\")[-1].replace('.', '-')\n",
    "# model_tar_dir = Path(f\"/home/ec2-user/SageMaker/models/{model_name}\")\n",
    "# print(\"model_tar_dir: \\n\", model_tar_dir)\n",
    "\n",
    "\n",
    "print(\"output_dir: \\n\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c9ce9997bd42dbb42c49d63f344f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356ad877bf2748438906fc9d7c631965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Model and run Inference\n",
    "\n",
    "After the training is done we want to evaluate and test our model. We will load different samples from the original dataset and evaluate the model manually. Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ce6998e528444cb2737527c3814bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd86604fbdb54f5e9b70a27069cd9ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "peft_model_id = \"./llama-3-70b-hf-no-robot\"\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = peft_model_id,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load our test dataset try to generate an instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [{'content': 'You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.',\n",
       "   'role': 'system'},\n",
       "  {'content': \"What are 5 things I can do when it's raining in London? I am visiting for the first time and only for a week with my husband. We love to walk, eat good food, and explore.\",\n",
       "   'role': 'user'}])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    "messages = eval_dataset[rand_idx][\"messages\"][:2]\n",
    "rand_idx, messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2675,    527,    445,  81101,     11,    459,  15592,  18328,   3549,\n",
       "            555,  20540,    311,    387,  11190,    323,  10978,     13,   4718,\n",
       "           6677,  45395,    264,   7029,   2134,    315,  13650,     11,  10923,\n",
       "            499,    311,  16988,    304,  67749,  21633,    323,   3493,   6492,\n",
       "            389,   6485,  15223,    382,  35075,     25,   3639,    527,    220,\n",
       "             20,   2574,    358,    649,    656,    994,    433,    596,  84353,\n",
       "            304,   7295,     30,    358,   1097,  17136,    369,    279,   1176,\n",
       "            892,    323,   1193,    369,    264,   2046,    449,    856,  10177,\n",
       "             13,   1226,   3021,    311,   4321,     11,   8343,   1695,   3691,\n",
       "             11,    323,  13488,     13, 128001,    271,  72803,     25,    220]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Query:**\n",
      "What are 5 things I can do when it's raining in London? I am visiting for the first time and only for a week with my husband. We love to walk, eat good food, and explore.\n",
      "\n",
      "**Original Answer:**\n",
      "Sure! Here are five options for things to do in London on a rainy day:\n",
      "\n",
      "1. Visit The British Museum. Dedicated to human history, art and culture, The British Museum has over 8 million permanent objects in its collection and is spread over multiple floors and wings of its impressive Bloomsbury building. From its extensive collection of Egyptian mummies to the Rosetta Stone, there are numerous exhibitions to capture your imagination. The museum is free to enjoy, though runs separately ticketed exhibitions throughout the year, as well. \n",
      "\n",
      "2. Explore Spitalfields Market. Since the 17th century, traders have been operating out of this East London market selling food, art, antiques, jewellery, and so much more. Today, the covered market is a hot destination for fashion and food enthusiasts with numerous stalls and shops open every day of the week. On the weekends, the market hosts various community events including pop-up music festivals. \n",
      "\n",
      "3. Shop at Westfield London. Located in West London, Westfield London is an enormous shopping centre hosting roughly 320 stores and restaurants spread over 2.5 million square feet, including the high-end retail sector named The Village. In addition to a wide selection of food and shopping, there are multiple activity options including miniature golf, rock climbing, and a cinema. \n",
      "\n",
      "4. Visit the Barbican Centre. The Barbican Centre is a large performing arts centre in the City of London nestled in the impressive Barbican Estate, which is an impressive example of Brutalist architecture. The centre is home to both the London Symphony Orchestra and the BBC Symphony Orchestra, in addition to hosting multiple theatre performances, film screenings, and additional concerts. The public are also welcome to enjoy its library, dine in one of its three restaurants, or explore its vast indoor conservatory. \n",
      "\n",
      "5. Explore the Battersea Power Station. The Grade-II listed Battersea Power Station, which sits on the south bank of the Thames just west of Westminster, was recently refurbished and reopened as a shopping and leisure destination. Inside the building, there are various shops, eateries, and bard including Control Room B: a cocktail bar nestled within the original dials and controls of the power station. If there's a break in the rainy weather, the riverfront of the power station is lined with additional food stalls, and Battersea Park is a short five-minute walk away down the bank of the Thames. If the rain stays all day, though, there are plenty of activities sheltered by the power station including miniature gold, ping pong, and a cinema. \n",
      "\n",
      "Whether it's rain or shine, London has an endless supply of activities, sights, and eateries to ensure you have an unforgettable trip. \n",
      "\n",
      "**Generated Answer:**\n",
      "1. The British Museum. The British Museum is the world's oldest national public museum. It was founded in 1753 and is located in the Bloomsbury area of London. The museum houses a collection of eight million works and is dedicated to human history, art, and culture. The museum is free to enter, but there are charges for some exhibitions and events. \n",
      "2. The Tower of London. The Tower of London is a historic castle on the north bank of the River Thames in central London. It was founded towards the end of the 11th century and is now a World Heritage Site. The Tower of London is one of the most popular tourist attractions in London. \n",
      "3. The London Eye. The London Eye is a giant Ferris wheel on the South Bank of the River Thames in London. It is the tallest Ferris wheel in Europe and the most popular paid tourist attraction in the United Kingdom, with over 3.75 million visitors annually. \n",
      "4. The National Gallery. The National Gallery is an art museum in Trafalgar Square in the City of Westminster, in Central London. It was founded in 1824 and houses a collection of over 2,300 paintings dating from the mid-13th century to the 1900s. \n",
      "5. The British Library. The British Library is the national library of the United Kingdom and the largest national library in the world by number of items cataloged. It is located on Euston Road in Somers Town, London, England, and is a legal deposit library, meaning that it receives copies of all books published in the United Kingdom and Ireland. The library also houses the world's largest collection of maps, with over 4.5 million items. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test on sample \n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(f\"**Query:**\\n{eval_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"**Original Answer:**\\n{eval_dataset[rand_idx]['messages'][2]['content']}\\n\")\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n",
    "# **Query:**\n",
    "# How long was the Revolutionary War?\n",
    "# **Original Answer:**\n",
    "# The American Revolutionary War lasted just over seven years. The war started on April 19, 1775, and ended on September 3, 1783. \n",
    "# **Generated Answer:**\n",
    "# The Revolutionary War, also known as the American Revolution, was an 18th-century war fought between the Kingdom of Great Britain and the Thirteen Colonies. The war lasted from 1775 to 1783."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! 🚀 Now, its your turn! \n",
    "\n",
    "If you want to deploy your model into production check out [Deploy the LLM for Production](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#6-deploy-the-llm-for-production)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
